A clone of Micrograd built by me for learning about the internal working of neural nets while following Andrej Karpathy's educational content.

### Micrograd
A tiny Autograd engine. Implements backpropagation (reverse-mode autodiff) over a dynamically built DAG and a small neural networks library on top of it with a PyTorch-like API. Original repo:- https://github.com/karpathy/micrograd

## Features
- Scalar-based automatic differentiation engine
- Dynamic computation graph construction
- Reverse-mode backpropagation
- PyTorch-like API
- Simple neural network modules (MLP, Neuron, Layer)
- Training with gradient descent


## Motivation
This project was built to deeply understand reverse-mode automatic differentiation and the internal mechanics of neural networks by implementing them from scratch, following Andrej Karpathyâ€™s educational content.
